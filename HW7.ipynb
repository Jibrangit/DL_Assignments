{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW7.ipynb",
      "provenance": [],
      "mount_file_id": "1fva080bcA2VRwc3zp5ehph8t5Fj-1XIJ",
      "authorship_tag": "ABX9TyNUBP4G8+JWpdo+aDTRoOeA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jibrangit/DL_Assignments/blob/Experiment/HW7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bRgUGohNTiT"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as Func\n",
        "import scipy"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNnRDtNHP6ql"
      },
      "source": [
        "INPUT = 784\n",
        "HIDDEN = 200\n",
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 100"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7gzNhlJQDO7"
      },
      "source": [
        "class Encoder (nn.Module):    \n",
        "  def __init__ (self):        \n",
        "    super(Encoder, self).__init__()        # TODO initialize layers \n",
        "    self.hidden = nn.Linear(784, HIDDEN) \n",
        "    self.output = nn.Linear(HIDDEN, 32)   \n",
        "    self.dropout = nn.Dropout()\n",
        "    self.activation = nn.Tanh() \n",
        "    # self.positive = nn.Sigmoid()  \n",
        "    self.flatten = nn.Flatten()\n",
        "    \n",
        "  def forward (self, X):        \n",
        "      # TODO execute layers and return result \n",
        "      X = self.flatten(X)\n",
        "      X = self.activation(X)\n",
        "      X = self.dropout(X)\n",
        "      X = self.hidden(X)\n",
        "      X = self.activation(X)\n",
        "      X = self.dropout(X)\n",
        "      MU = self.output(X)   \n",
        "      SIGMA2 = self.output(X)   \n",
        "      SIGMA2 = torch.square(SIGMA2)\n",
        "      \n",
        "      return MU, SIGMA2\n",
        "\n",
        "class Decoder (nn.Module):    \n",
        "  def __init__ (self):        \n",
        "    super(Decoder, self).__init__()        \n",
        "    # TODO initialize layers\n",
        "    self.hidden = nn.Linear(32, HIDDEN) \n",
        "    self.output = nn.Linear(HIDDEN, 784)   \n",
        "    self.dropout = nn.Dropout()\n",
        "    self.activation = nn.Tanh()   \n",
        "    self.sigmoid_output = nn.Sigmoid()    \n",
        "  \n",
        "  def forward (self, Z):        \n",
        "    # TODO execute layers and return result \n",
        "    Z = self.hidden(Z)\n",
        "    Z = self.activation(Z)\n",
        "    Z = self.dropout(Z)\n",
        "    Z = self.output(Z)   \n",
        "    Z = self.sigmoid_output(Z)    \n",
        "    \n",
        "    \n",
        "    return Z\n",
        "\n",
        "class VAE (nn.Module):    \n",
        "  def __init__ (self):        \n",
        "    super(VAE, self).__init__()        \n",
        "    self.decoder = Decoder()        \n",
        "    self.encoder = Encoder()\n",
        "\n",
        "# Computes the hidden representation given the pre-sampled values eps    \n",
        "  def latent_code (self, x):        \n",
        "    mu, sigma2 = self.encoder(x)    \n",
        "    # TODO extract mu and sigma2 from the code, and then use them and eps        \n",
        "    # to compute z.\n",
        "    sigma = torch.sqrt(sigma2) \n",
        "    epsilon = torch.LongTensor(scipy.random.normal(0, 1, sigma.shape[1]))\n",
        "    # print(sigma.shape)\n",
        "    z =  mu.mul(epsilon) + sigma\n",
        "\n",
        "\n",
        "    return z, mu, sigma2\n",
        "\n",
        "\n",
        "  def recon_x (self, z):        \n",
        "     x = self.decoder(z)        \n",
        "     return x\n",
        "\n",
        "def train (device, vae, x):\n",
        "  optimizer = optim.Adam(vae.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "  losses = []\n",
        "  def computeLoss (x): \n",
        "    # TODO implement your custom loss \n",
        "    z, mu, sigma2 = vae.latent_code(x)\n",
        "    reconstructed_x = vae.recon_x(z)\n",
        "    # print(f' reconstructed x is: {reconstructed_x[0]}')\n",
        "    # print(f' x is: {x[0]}')\n",
        "    # print(f'Shapes of x and reconstructed x are: {x.shape} and {reconstructed_x.shape}')\n",
        "\n",
        "    BCE =  Func.binary_cross_entropy(reconstructed_x, x, reduction='sum')\n",
        "    # KLD = -0.5 * torch.sum(1 + torch.log(sigma2) - mu.pow(2) - sigma2.pow(2))\n",
        "    KLD = 0\n",
        "\n",
        "    Total_loss = BCE + KLD\n",
        "    return Total_loss, z\n",
        "\n",
        "  for e in range(NUM_EPOCHS):\n",
        "    if e%10 == 0:\n",
        "      print(\"Epoch: {}\".format(e))\n",
        "    for i in np.arange(0, len(X), BATCH_SIZE):\n",
        "      # Call computeLoss on each minibatch\n",
        "      miniBatch = X[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
        "      loss, z = computeLoss(miniBatch)         \n",
        "      loss.backward()            \n",
        "      optimizer.step()    "
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O9YzSQbRHdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "45611431-70dc-491d-e4cb-228e91062070"
      },
      "source": [
        "if __name__ == \"__main__\":    \n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
        "  vae = VAE().to(device)    \n",
        "  X = torch.from_numpy(np.load(\"/content/drive/MyDrive/Colab Notebooks/DL_Assignments/HW7/mnist_train_images.npy\")).float().to(device)   \n",
        "  # print(X.shape) \n",
        "  train(device, vae, X)\n",
        "\n",
        "  # z_sample = vae.encoder.forward(X[0:16])\n",
        "  # z_sample = torch.LongTensor(z_sample)\n",
        "  # recon_x_sample = vae.decoder(z_sample)\n",
        "  # print(recon_x.sample.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(32)\n",
        "    sample = vae.decoder(z)\n",
        "    \n",
        "    Z = sample.reshape([28, 28])\n",
        "    plt.imshow(Z, cmap=\"gray\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Epoch: 10\n",
            "Epoch: 20\n",
            "Epoch: 30\n",
            "Epoch: 40\n",
            "Epoch: 50\n",
            "Epoch: 60\n",
            "Epoch: 70\n",
            "Epoch: 80\n",
            "Epoch: 90\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}